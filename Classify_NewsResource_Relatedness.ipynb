{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets import usual suspects\n",
    "import requests\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from math import log\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWashingtonPost(url,token):\n",
    "    # THis function takes the URL of an article in the \n",
    "    # Washington Post, and then returns the article minus all \n",
    "    # of the crud - HTML, javascript etc. How? By searching for\n",
    "    # everything that lies between the tags titled 'token'\n",
    "    # Like most web-scraping, this will only work for urls where\n",
    "    # we know the structure (eg say all articles in the WashPo are\n",
    "    # enclosed in <article></article> tags). This will also change from\n",
    "    # time to time as different HTML formats are employed in the website\n",
    "    try:\n",
    "        page = urllib2.urlopen(url).read().decode('utf8')\n",
    "    except:\n",
    "        # if unable to download the URL, return title = None, article = None\n",
    "        return (None,None)\n",
    "    soup = BeautifulSoup(page)\n",
    "    if soup is None:\n",
    "        return (None,None)\n",
    "    # If we are here, it means the error checks were successful, we were\n",
    "    # able to parse the page\n",
    "    text = \"\"\n",
    "    if soup.find_all(token) is not None:\n",
    "        # Search the page for whatever token demarcates the article\n",
    "        # usually '<article></article>'\n",
    "        text = ''.join(map(lambda p: p.text, soup.find_all(token)))\n",
    "        # mush together all the text in the <article></article> tags\n",
    "        soup2 = BeautifulSoup(text)\n",
    "        # create a soup of the text within the <article> tags\n",
    "        if soup2.find_all('p')!=[]:\n",
    "            # now mush together the contents of what is in <p> </p> tags\n",
    "            # within the <article>\n",
    "            text = ''.join(map(lambda p: p.text, soup2.find_all('p')))\n",
    "    return text, soup.title.text\n",
    "    # what did we just do? Let's go through and understand\n",
    "    # finally return the result tuple with the title and the body of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now we will do something very very similar, but this time for the New York Times\n",
    "def getNewYorkTimes(url,token):\n",
    "    response = requests.get(url)\n",
    "    # THis is an alternative way to get the contents of a URL\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    page = str(soup)\n",
    "    title = soup.find('title').text\n",
    "    mydivs = soup.findAll(\"p\", {\"class\":\"story-body-text story-content\"})\n",
    "    text = ''.join(map(lambda p:p.text, mydivs))\n",
    "    return text, title\n",
    "    # Notice again how important it is to know the structure of the page\n",
    "    # we are seeking to scrape. If we did not know that articles in the NYT\n",
    "    # come contained in these tags - an outer tag <p> and an inner tag\n",
    "    # of class = story-body-text story-content, we would be unable to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok! Now we have a way to extract the contents and title of an individual\n",
    "# URL. Let's hook this up inside another function that will take the URL\n",
    "# of an entire section of a newspaper - say the Technology or Sports section\n",
    "# of a newspaper - and parse all of the URLs for articles linked off that\n",
    "# section. \n",
    "# Btw, these sections also come with plenty of non-news links - 'about',\n",
    "# how to syndicate etc, so we will employ a little hack - we will consider\n",
    "# something to be a news article only if the url has a dateline. THis is \n",
    "# actually very safe - its pretty much the rule for articles to have a \n",
    "# date, and virtually all important newspapers mush this date into the URL.\n",
    "def scrapContent(url, year='2015',contentSourceFunction=getNewYorkTimes,token='None'):\n",
    "    urlBodies = {}\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    # we are set up with a Soup of the page - now find the links\n",
    "    # Remember that links are always of the form \n",
    "    # <a href='link-url'> link-text </a>\n",
    "    numErrors = 0\n",
    "    for a in soup.findAll('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            if( (url not in urlBodies) and \n",
    "               ((year is not None and year in url) \n",
    "               or year is None)):\n",
    "                body = contentSourceFunction(url,token)\n",
    "                # this line above is important - contentSourceFunction \n",
    "                # refers to the individual scraper function for the \n",
    "                # new york times or the washington post etc.\n",
    "                if body and len(body) > 0:\n",
    "                    urlBodies[url] = body\n",
    "                print url\n",
    "        except:\n",
    "            numErrors += 1\n",
    "            # plenty of parse errors happen - links might not\n",
    "            # be external links, might be malformed and so on -\n",
    "            # so don't mind if there are exceptions.\n",
    "    return urlBodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now for the frequency summarizer class - which we have encountered\n",
    "# before. To quickly jog our memories - given an (title,article-body) tuple\n",
    "# the frequency summarizer has easy ways to find the most 'important'\n",
    "# sentences, and the most important words. How is 'important' defined?\n",
    "# Important = most frequent, excluding 'stopwords' which are generic\n",
    "# words like 'the' etc which can be ignored\n",
    "class FrequencySummarizer:\n",
    "    def __init__(self,min_cut=0.1,max_cut=0.9):\n",
    "        # class constructor - takes in min and max cutoffs for \n",
    "        # frequency\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut\n",
    "        self._stopwords = set(stopwords.words('english') +\n",
    "                              list(punctuation) +\n",
    "                              [u\"'s\",'\"'])\n",
    "        # notice how the stopwords are a set, not a list. \n",
    "        # its easy to go from set to list and vice-versa\n",
    "        # (simply use the set() and list() functions) - \n",
    "        # but conceptually sets are different from lists\n",
    "        # because sets don't have an order to their elements\n",
    "        # while lists do\n",
    "    \n",
    "    def _compute_frequencies(self,word_sent,customStopWords=None):\n",
    "        freq = defaultdict(int)\n",
    "        # we have encountered defaultdict objects before\n",
    "        if customStopWords is None:\n",
    "            stopwords = set(self._stopwords)\n",
    "        else:\n",
    "            stopwords = set(customStopWords).union(self._stopwords)\n",
    "        for sentence in word_sent:\n",
    "            for word in sentence:\n",
    "                if word not in stopwords:\n",
    "                    freq[word] += 1\n",
    "        m = float(max(freq.values()))\n",
    "        for word in freq.keys():\n",
    "            freq[word] = freq[word]/m\n",
    "            if freq[word] >= self._max_cut or freq[word] <= self._min_cut:\n",
    "                del freq[word]\n",
    "        return freq\n",
    "    \n",
    "    def extractFeatures(self,article,n,customStopWords=None):\n",
    "        # The article is passed in as a tuple (text, title)\n",
    "        text = article[0]\n",
    "        # extract the text\n",
    "        title = article[1]\n",
    "        # extract the title\n",
    "        sentences = sent_tokenize(text)\n",
    "        # split the text into sentences\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sentences]\n",
    "        # split the sentences into words \n",
    "        self._freq = self._compute_frequencies(word_sent,customStopWords)\n",
    "        # calculate the word frequencies using the member function above\n",
    "        if n < 0:\n",
    "            # how many features (words) to return? IF the user has\n",
    "            # asked for a negative number, this is a sign that we don't\n",
    "            # do any feature selection - we return ALL features\n",
    "            # THis is feature extraction without any pruning, ie no\n",
    "            # feature selection (beyond simply picking words as the features)\n",
    "            return nlargest(len(self._freq_keys()),self._freq,key=self._freq.get)\n",
    "        else:\n",
    "            # if the calling function has asked for a subset then\n",
    "            # return only the 'n' largest features - ie here the most\n",
    "            # important words (important == frequent, barring stopwords)\n",
    "            return nlargest(n,self._freq,key=self._freq.get)\n",
    "        # let's summarize what we did here. \n",
    "    \n",
    "    def extractRawFrequencies(self, article):\n",
    "        # very similar, except that this method will return the 'raw'\n",
    "        # frequencies - literally just the word counts\n",
    "        text = article[0]\n",
    "        title = article[1]\n",
    "        sentences = sent_tokenize(text)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sentences]\n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "        return freq\n",
    "    \n",
    "    def summarize(self, article,n):\n",
    "        text = article[0]\n",
    "        title = article[1]\n",
    "        sentences = sent_tokenize(text)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sentences]\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i,sentence in enumerate(word_sent):\n",
    "            for word in sentence:\n",
    "                if word in self._freq:\n",
    "                    ranking[i] += self._freq[word]\n",
    "        sentences_index = nlargest(n,ranking,key=ranking.get)\n",
    "\n",
    "        return [sentences[j] for j in sentences_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urlWashingtonPostNonTech = \"https://www.washingtonpost.com/sports/\"\n",
    "urlNewYorkTimesNonTech = \"https://www.nytimes.com/pages/sports/index.html\"\n",
    "urlWashingtonPostTech = \"https://www.washingtonpost.com/business/technology/\"\n",
    "urlNewYorkTimesTech = \"https://www.nytimes.com/pages/technology/index.html\"\n",
    "\n",
    "washingtonPostTechArticles = scrapContent(urlWashingtonPostTech,'2016',getWashingtonPost,'article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print len(washingtonPostTechArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.washingtonpost.com/news/nationals-journal/wp/2017/01/22/max-scherzer-receives-2016-cy-young-award/\n"
     ]
    }
   ],
   "source": [
    "washingtonPostNonTechArticles = scrapContent(urlWashingtonPostNonTech,'2016',getWashingtonPost,'article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print len(washingtonPostNonTechArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newYorkTimesTechArticles = scrapContent(urlNewYorkTimesTech,'2016', getNewYorkTimes, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print len(newYorkTimesTechArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newYorkTimesNonTechArticles = scrapContent(urlNewYorkTimesNonTech, '2016', getNewYorkTimes, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print len(newYorkTimesNonTechArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's collect these article summaries in an easy to classify form\n",
    "articleSummaries = {}\n",
    "for techUrlDictionary in [newYorkTimesTechArticles, washingtonPostTechArticles]:\n",
    "    for articleUrl in techUrlDictionary:\n",
    "        if techUrlDictionary[articleUrl][0] is not None:\n",
    "            if len(techUrlDictionary[articleUrl][0]) > 0:\n",
    "                fs = FrequencySummarizer()\n",
    "                summary = fs.extractFeatures(techUrlDictionary[articleUrl],25)\n",
    "                articleSummaries[articleUrl] = {'feature-vector': summary,\n",
    "                                               'label': 'Tech'}\n",
    "for nontechUrlDictionary in [newYorkTimesNonTechArticles, washingtonPostNonTechArticles]:\n",
    "    for articleUrl in nontechUrlDictionary:\n",
    "        if nontechUrlDictionary[articleUrl][0] is not None:\n",
    "            if len(nontechUrlDictionary[articleUrl][0]) > 0:\n",
    "                fs = FrequencySummarizer()\n",
    "                summary = fs.extractFeatures(nontechUrlDictionary[articleUrl],25)\n",
    "                articleSummaries[articleUrl] = {'feature-vector': summary,\n",
    "                                               'label': 'Non-Tech'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://www.washingtonpost.com/news/nationals-journal/wp/2017/01/22/max-scherzer-receives-2016-cy-young-award/': {'feature-vector': [u'league', u'young', u'thanked', u'cy', u'national', u'one', u'award', u'american', u'\\u201d', u'\\u2014', u'thank', u'game', u'nationals', u'year', u'winning', u'seager', u'2016', u'honorees', u'roberts', u'said', u'new', u'baseball', u'york', u'awards', u'course'], 'label': 'Non-Tech'}}\n"
     ]
    }
   ],
   "source": [
    "print articleSummaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDoxyDonkeyText(testUrl,token):\n",
    "    response = requests.get(testUrl)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    page = str(soup)\n",
    "    title = soup.find(\"title\").text\n",
    "    mydivs = soup.findAll(\"div\", {\"class\":token})\n",
    "    text = ''.join(map(lambda p:p.text,mydivs))\n",
    "    return text,title\n",
    "    # our test instance, just like our training data, is nicely\n",
    "    # setup as a (title,text) tuple\n",
    "\n",
    "testUrl = \"http://doxydonkey.blogspot.in\"\n",
    "testArticle = getDoxyDonkeyText(testUrl,\"post-body\")\n",
    "\n",
    "fs = FrequencySummarizer()\n",
    "testArticleSummary = fs.extractFeatures(testArticle, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Non-Tech']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = {}\n",
    "for articleUrl in articleSummaries:\n",
    "    oneArticleSummary = articleSummaries[articleUrl]['feature-vector']\n",
    "    similarities[articleUrl] = len(set(testArticleSummary).intersection(set(oneArticleSummary)))\n",
    "\n",
    "labels = defaultdict(int)    \n",
    "knn = nlargest(5, similarities, key=similarities.get)\n",
    "for oneNeighbor in knn:\n",
    "    labels[articleSummaries[oneNeighbor]['label']] += 1\n",
    "\n",
    "nlargest(1,labels,key=labels.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cumulativeRawFrequencies = {'Tech':defaultdict(int),'Non-Tech':defaultdict(int)}\n",
    "trainingData = {'Tech':newYorkTimesTechArticles,'Non-Tech':newYorkTimesNonTechArticles}\n",
    "for label in trainingData:\n",
    "    for articleUrl in trainingData[label]:\n",
    "        if len(trainingData[label][articleUrl][0]) > 0:\n",
    "            fs = FrequencySummarizer()\n",
    "            rawFrequencies = fs.extractRawFrequencies(trainingData[label][articleUrl])\n",
    "            for word in rawFrequencies:\n",
    "                cumulativeRawFrequencies[label][word] += rawFrequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print cumulativeRawFrequencies['Non-Tech'].values()\n",
    "print cumulativeRawFrequencies['Tech'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9d2d20fa2da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# non-tech articles respectively, as a proportion of the total number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtechiness\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulativeRawFrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulativeRawFrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulativeRawFrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Non-Tech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mnontechiness\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulativeRawFrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Non-Tech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulativeRawFrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulativeRawFrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Non-Tech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtechiness\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnontechiness\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "techiness = 1.0\n",
    "nontechiness = 1.0\n",
    "for word in testArticleSummary:\n",
    "    # for each 'feature' of the test instance - \n",
    "    if word in cumulativeRawFrequencies['Tech']:\n",
    "        techiness *= 1e3*cumulativeRawFrequencies['Tech'][word] / float(sum(cumulativeRawFrequencies['Tech'].values()))\n",
    "        # we multiply the techiness by the probability of this word\n",
    "        # appearing in a tech article (based on the training data)\n",
    "    else:\n",
    "        techiness /= 1e3\n",
    "        # THis is worth paying attention to. If the word does not appear\n",
    "        # in the tech articles of the training data at all,we could simply\n",
    "        # set that probability to zero - in fact doing so is the 'correct'\n",
    "        # way mathematically, because that way all of the probabilities would\n",
    "        # sum to 1. But that would lead to 'snap' decisions since the techiness\n",
    "        # would instantaneously become 0. To prevent this, we decide to take\n",
    "        # the probability as some very small number (here 1 in 1000, which is \n",
    "        # actually not all that low)\n",
    "    # Now the exact same deal- but for the nontechiness. We are intentionally\n",
    "    # copy-pasting code (not a great software development practice) in order\n",
    "    # to make the logic very clear. Ideally, we would have created a function\n",
    "    # and called it twice rather than copy-pasting this code. In any event..\n",
    "    if word in cumulativeRawFrequencies['Non-Tech']:\n",
    "        nontechiness *= 1e3*cumulativeRawFrequencies['Non-Tech'][word] / float(sum(cumulativeRawFrequencies['Non-Tech'].values()))\n",
    "        # we multiply the techiness by the probability of this word\n",
    "        # appearing in a tech article (based on the training data)\n",
    "    else:\n",
    "        nontechiness /= 1e3\n",
    "\n",
    "# we are almost done! Now we simply need to scale the techiness \n",
    "# and non-techiness by the probabilities of overall techiness and\n",
    "# non-techiness. THis is simply the number of words in the tech and \n",
    "# non-tech articles respectively, as a proportion of the total number\n",
    "# of words\n",
    "techiness *= float(sum(cumulativeRawFrequencies['Tech'].values())) / (float(sum(cumulativeRawFrequencies['Tech'].values())) + float(sum(cumulativeRawFrequencies['Non-Tech'].values())))\n",
    "nontechiness *= float(sum(cumulativeRawFrequencies['Non-Tech'].values())) / (float(sum(cumulativeRawFrequencies['Tech'].values())) + float(sum(cumulativeRawFrequencies['Non-Tech'].values())))\n",
    "if techiness > nontechiness:\n",
    "    label = 'Tech'\n",
    "else:\n",
    "    label = 'Non-Tech'\n",
    "print label, techiness, nontechiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllDoxyDonkeyPosts(url,links):\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    for a in soup.findAll('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            title = a['title']\n",
    "            if title == \"Older Posts\":\n",
    "                print title, url\n",
    "                links.append(url)\n",
    "                getAllDoxyDonkeyPosts(url,links)\n",
    "        except:\n",
    "            title = \"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2017-01-16T17:56:00-08:00&max-results=7\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2017-01-05T18:06:00-08:00&max-results=7&start=7&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-12-27T19:06:00-08:00&max-results=7&start=14&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-12-18T18:39:00-08:00&max-results=7&start=21&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-12-07T19:33:00-08:00&max-results=7&start=28&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-09-20T19:58:00-07:00&max-results=7&start=35&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-09-01T21:18:00-07:00&max-results=7&start=42&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-08-22T20:09:00-07:00&max-results=7&start=49&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-08-10T22:08:00-07:00&max-results=7&start=56&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-08-01T19:12:00-07:00&max-results=7&start=63&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-07-20T19:12:00-07:00&max-results=7&start=70&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-07-11T19:16:00-07:00&max-results=7&start=77&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-06-29T20:03:00-07:00&max-results=7&start=84&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-06-20T19:44:00-07:00&max-results=7&start=91&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-06-09T19:21:00-07:00&max-results=7&start=98&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-05-31T20:02:00-07:00&max-results=7&start=105&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-05-22T20:03:00-07:00&max-results=7&start=112&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-05-10T19:41:00-07:00&max-results=7&start=119&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-05-02T19:23:00-07:00&max-results=7&start=126&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-04-22T19:25:00-07:00&max-results=7&start=133&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-04-13T19:36:00-07:00&max-results=7&start=140&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-04-04T18:53:00-07:00&max-results=7&start=147&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-03-23T19:16:00-07:00&max-results=7&start=154&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-03-14T18:54:00-07:00&max-results=7&start=161&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-03-02T18:13:00-08:00&max-results=7&start=168&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-02-22T18:15:00-08:00&max-results=7&start=175&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-02-11T18:01:00-08:00&max-results=7&start=182&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-02-02T18:11:00-08:00&max-results=7&start=189&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-01-21T18:27:00-08:00&max-results=7&start=196&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-01-12T18:08:00-08:00&max-results=7&start=203&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2016-01-03T18:19:00-08:00&max-results=7&start=210&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-12-21T18:58:00-08:00&max-results=7&start=217&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-12-10T18:32:00-08:00&max-results=7&start=224&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-12-01T18:22:00-08:00&max-results=7&start=231&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-11-22T18:15:00-08:00&max-results=7&start=238&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-11-11T18:45:00-08:00&max-results=7&start=245&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-11-01T18:53:00-08:00&max-results=7&start=252&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-10-15T19:30:00-07:00&max-results=7&start=259&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-10-06T19:14:00-07:00&max-results=7&start=266&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-09-23T19:59:00-07:00&max-results=7&start=273&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-09-14T19:25:00-07:00&max-results=7&start=280&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-09-03T19:25:00-07:00&max-results=7&start=287&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-08-25T19:06:00-07:00&max-results=7&start=294&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-08-16T19:20:00-07:00&max-results=7&start=301&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-08-05T19:26:00-07:00&max-results=7&start=308&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-07-27T20:03:00-07:00&max-results=7&start=315&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-07-16T19:36:00-07:00&max-results=7&start=322&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-07-07T19:29:00-07:00&max-results=7&start=329&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-06-28T21:20:00-07:00&max-results=7&start=336&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-06-17T18:47:00-07:00&max-results=7&start=343&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-06-08T19:02:00-07:00&max-results=7&start=350&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-05-27T19:40:00-07:00&max-results=7&start=357&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-05-18T19:22:00-07:00&max-results=7&start=364&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-05-07T20:18:00-07:00&max-results=7&start=371&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-04-28T21:02:00-07:00&max-results=7&start=378&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-04-19T20:09:00-07:00&max-results=7&start=385&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-04-08T20:42:00-07:00&max-results=7&start=392&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-03-29T20:39:00-07:00&max-results=7&start=399&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-03-18T20:31:00-07:00&max-results=7&start=406&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-03-09T20:41:00-07:00&max-results=7&start=413&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-02-25T19:35:00-08:00&max-results=7&start=420&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-02-16T19:50:00-08:00&max-results=7&start=427&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-02-05T20:03:00-08:00&max-results=7&start=434&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-01-27T19:41:00-08:00&max-results=7&start=441&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-01-19T19:35:00-08:00&max-results=7&start=446&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2015-01-11T19:32:00-08:00&max-results=7&start=452&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2014-12-30T19:35:00-08:00&max-results=7&start=459&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2014-12-18T19:49:00-08:00&max-results=7&start=466&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2014-12-09T18:59:00-08:00&max-results=7&start=473&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2014-12-01T01:50:00-08:00&max-results=7&start=480&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2014-11-20T01:42:00-08:00&max-results=7&start=487&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2014-11-11T01:39:00-08:00&max-results=7&start=494&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2014-10-31T01:26:00-07:00&max-results=7&start=501&by-date=false\n",
      "Older Posts http://doxydonkey.blogspot.in/search?updated-max=2014-10-22T01:22:00-07:00&max-results=7&start=508&by-date=false\n"
     ]
    }
   ],
   "source": [
    "blogUrl = \"http://doxydonkey.blogspot.in\"\n",
    "links = []\n",
    "getAllDoxyDonkeyPosts(blogUrl,links)\n",
    "doxyDonkeyPosts = {}\n",
    "for link in links:\n",
    "    doxyDonkeyPosts[link] = getDoxyDonkeyText(link,'post-body')\n",
    "\n",
    "\n",
    "documentCorpus = []\n",
    "for onePost in doxyDonkeyPosts.values():\n",
    "    documentCorpus.append(onePost[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 117.308\n",
      "Iteration  1, inertia 60.867\n",
      "Converged at iteration 1\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/pankajmathur/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7bad070693cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moneDocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocumentCorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrequencySummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     summary = fs.extractFeatures((oneDocument,\"\"),\n\u001b[1;32m     13\u001b[0m                                 \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f31205f40e2a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, min_cut, max_cut)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_cut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_cut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         self._stopwords = set(stopwords.words('english') +\n\u001b[0;32m---> 14\u001b[0;31m                               \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                               [u\"'s\",'\"'])\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# notice how the stopwords are a set, not a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pankajmathur/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pankajmathur/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/pankajmathur/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "# In[14]:\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')\n",
    "X = vectorizer.fit_transform(documentCorpus)\n",
    "km = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)\n",
    "km.fit(X)\n",
    "\n",
    "keywords = {}\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    oneDocument = documentCorpus[i]\n",
    "    fs = FrequencySummarizer()\n",
    "    summary = fs.extractFeatures((oneDocument,\"\"),\n",
    "                                100,\n",
    "                                [u\"according\",u\"also\",u\"billion\",u\"like\",u\"new\", u\"one\",u\"year\",u\"first\",u\"last\"])\n",
    "    if cluster not in keywords:\n",
    "        keywords[cluster] = set(summary)\n",
    "    else:\n",
    "        keywords[cluster] = keywords[cluster].intersection(set(summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
